{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import h5py\n",
    "from scipy.stats import norm\n",
    "from keras import *\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation,Input, Dense, Dropout, merge, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.optimizers import *\n",
    "from keras.models import model_from_json, save_model\n",
    "from keras.layers import *\n",
    "import matplotlib.pyplot as pt\n",
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadmodel(name, weights = False):\n",
    "    json_file = open('/home/kaustuv1993/Desktop/H2bb/Models_and_Weights/%s_m.json'%name, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    #load weights into new model\n",
    "    if weights==True:\n",
    "        model.load_weights('/home/kaustuv1993/Desktop/H2bb/Models_and_Weights/%s_w.h5'%name)\n",
    "    print (model.summary())\n",
    "    print(\"Loaded model %s from disk\"%name)\n",
    "    return model\n",
    "\n",
    "def savemodel(model,name=\"neural network\"):\n",
    "    #print \"Saving model:\"\n",
    "    model_name = name\n",
    "    #model.summary()\n",
    "    model.save_weights('/home/kaustuv1993/Desktop/H2bb/Models_and_Weights/%s_w.h5'%model_name, overwrite=True)\n",
    "    model_json = model.to_json()\n",
    "    with open(\"/home/kaustuv1993/Desktop/H2bb/Models_and_Weights/%s_m.json\"%model_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "        \n",
    "def savelosses(hist, name=\"neural network\"):  \n",
    "    print \"Saving losses:\"\n",
    "    loss = np.array(hist.history['loss'])\n",
    "    valoss = np.array(hist.history['val_loss'])\n",
    "    f = h5py.File(\"/home/kaustuv1993/Desktop/H2bb/Models_and_Weights/%s_h.h5\"%name,\"w\")\n",
    "    f.create_dataset('loss',data=loss)\n",
    "    f.create_dataset('val_loss',data=valoss)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "def get_cmap(N):\n",
    "    \n",
    "    '''Returns a function that maps each index in 0, 1, ... N-1 to a distinct \n",
    "    RGB color.'''\n",
    "    color_norm  = colors.Normalize(vmin=0, vmax=N-1)\n",
    "    scalar_map = cmx.ScalarMappable(norm=color_norm, cmap='hsv') \n",
    "    def map_index_to_rgb_color(index):\n",
    "        return scalar_map.to_rgba(index)\n",
    "    return map_index_to_rgb_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hist(y, n=25):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    l = y.min()\n",
    "    r = y.max()\n",
    "    w = ((r-l)/float(y.shape[0]*1.))\n",
    "    plt.hist(y, histtype='step', bins=n)\n",
    "    plt.show()\n",
    "\n",
    "#generates a Gaussian \n",
    "def gaussian(batch_size, mu=0.,sigma=0.5):\n",
    "    sample = np.random.normal(mu, sigma, batch_size)\n",
    "    sample.sort()\n",
    "    return np.reshape(sample,(len(sample),1))\n",
    "\n",
    "def noise(batch_size, m=-2.5, M=2.5):\n",
    "    a = np.linspace(m, M, batch_size) + np.random.random(batch_size) * 0.01\n",
    "    return np.reshape(a,(len(a),1))\n",
    "\n",
    "def noise_g(batch_size, mu=0.,sigma=1.0):\n",
    "    sample = np.random.normal(mu, sigma, batch_size)+np.random.random(batch_size) * 0.01\n",
    "    sample.sort()\n",
    "    return np.reshape(sample,(len(sample),1))\n",
    "\n",
    "#y=gaussian(batch_size=10000, mu=5, sigma=5)\n",
    "#print y.shape\n",
    "#hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(noise_g(1000, -2 , 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(losses,modelname=\"GAN\"):\n",
    "    #display.clear_output(wait=True)\n",
    "    #display.display(plt.gcf())\n",
    "    plt.figure(figsize=(16,9))\n",
    "    plt.plot(losses[\"d_f\"], label='discriminative loss on fake', alpha=0.8)\n",
    "    plt.plot(losses[\"d_t\"], label='discriminative loss on true', alpha=0.8)\n",
    "    plt.plot(losses[\"d\"], label = 'discriminative loss', alpha=0.8)\n",
    "    plt.plot(losses[\"g\"], label='generative loss', alpha=0.8)\n",
    "    #plt.yscale('log')\n",
    "    plt.xlabel('Epochs', fontsize=22)\n",
    "    plt.ylabel('Loss', fontsize=22)\n",
    "    plt.legend(fontsize=22, loc='best', fancybox=True, framealpha=0.)\n",
    "    plt.savefig(\"%s_loss.png\"%modelname)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=1e-4)#, decay = 0.009)\n",
    "dopt = Adam(lr=1e-3)#, decay = 0.0009)\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import concatenate, merge\n",
    "def gen_f():\n",
    "    \n",
    "    g_input= Input(shape=[1])\n",
    "    mean_input = Input(shape=[1])\n",
    "    g = merge([g_input, mean_input])\n",
    "    g = Dense(800)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    #g = Dropout(0.2)(g)\n",
    "    g = Dense(400)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    #g = Dropout(0.2)(g)\n",
    "    g = Dense(400)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = Dense(400)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    #g = Dropout(0.2)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = Dense(1, activation='linear')(g)\n",
    "    G = Model(inputs=[g_input, mean_input], outputs=[g] )\n",
    "    G.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return G\n",
    "\n",
    "#def disc_f():\n",
    "\n",
    "#    d_input= Input(shape=[1])\n",
    "#    d = Dense(400)(d_input)\n",
    "#    d = Activation('tanh')(d)\n",
    "#    d = BatchNormalization()(d)\n",
    "#    d = Dense(400)(d)\n",
    "#    d = Activation('tanh')(d)\n",
    "    #d = Dense(400)(d)\n",
    "    #d = Activation('tanh')(d)\n",
    "    #d = Flatten()(d)\n",
    "#    d = MinibatchDiscrimination(nb_kernels=10, kernel_dim=1)(d)#, input_shape=(None, 400),)(d)\n",
    "    #minibatch_featurizer = Lambda(minibatch_discriminator,\n",
    "                                     # output_shape=minibatch_output_shape)\n",
    "    #features = [d]\n",
    "    \n",
    "    #nb_features = 10\n",
    "    #vspace_dim = 2\n",
    "\n",
    "    # creates the kernel space for the minibatch discrimination\n",
    "    #K_x = Dense3D(nb_features, vspace_dim)(x)\n",
    "    #features.append(Activation('tanh')(minibatch_featurizer(K_x)))\n",
    "    \n",
    "    \n",
    "    \n",
    "#    d = Dense(1, kernel_initializer='normal', activation='sigmoid')(d)\n",
    "#    D = Model(d_input,d)\n",
    "#    D.compile(loss='binary_crossentropy', optimizer=dopt)\n",
    "#    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##From CaloGAN paper\n",
    "from ops_CaloGAN import *\n",
    "def minibatch_discriminator(x):\n",
    "    \"\"\" Computes minibatch discrimination features from input tensor x\"\"\"\n",
    "    diffs = K.expand_dims(x, 3) - K.expand_dims(K.permute_dimensions(x, [1, 2, 0]), 0)\n",
    "    l1_norm = K.sum(K.abs(diffs), axis=2)\n",
    "    return K.sum(K.exp(-l1_norm), axis=2)\n",
    "\n",
    "\n",
    "def minibatch_output_shape(input_shape):\n",
    "    \"\"\" Computes output shape for a minibatch discrimination layer\"\"\"\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  # only valid for 3D tensors\n",
    "    return tuple(shape[:2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_discriminator(d_input, mbd=False):\n",
    "    \"\"\" Generator sub-component for the CaloGAN ---- reusing this recipe for now (KD)\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        mdb: bool, perform feature level minibatch discrimination\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "        a keras tensor of features\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    d = Dense(800)(d_input)\n",
    "    d = Activation('tanh')(d)\n",
    "    d = Dropout(0.2)(d)\n",
    "    d = Dense(800)(d)\n",
    "    d = Activation('tanh')(d)\n",
    "    d = Dropout(0.2)(d)\n",
    "    d = Dense(400)(d)\n",
    "    d = Activation('tanh')(d)\n",
    "    d = Dropout(0.2)(d)\n",
    "    d = Dense(400)(d)\n",
    "    d = Activation('tanh')(d)\n",
    "    d = Dropout(0.2)(d)\n",
    "    #d = Dense(400)(d)\n",
    "    #d = Activation('tanh')(d)\n",
    "    #d = Dropout(0.2)(d)\n",
    "    #d = Dense(400)(d)\n",
    "    #d = Activation('tanh')(d)\n",
    "    #d = Dropout(0.2)(d)\n",
    "    #d = BatchNormalization()(d)\n",
    "    if mbd:\n",
    "        minibatch_featurizer = Lambda(minibatch_discriminator,\n",
    "                                      output_shape=minibatch_output_shape)\n",
    "\n",
    "        features = [d]\n",
    "        nb_features = 30\n",
    "        vspace_dim = 2\n",
    "\n",
    "        # creates the kernel space for the minibatch discrimination\n",
    "        K_d = Dense3D(nb_features, vspace_dim)(d)\n",
    "        features.append(Activation('tanh')(minibatch_featurizer(K_d)))\n",
    "        features = concatenate(features)\n",
    "        #p = concatenate([features])\n",
    "        D = Dense(1, kernel_initializer='normal', activation='sigmoid')(features)\n",
    "        discriminator = Model(d_input,D)\n",
    "        discriminator.compile(loss='binary_crossentropy', optimizer=dopt)\n",
    "        return discriminator\n",
    "    \n",
    "    else:\n",
    "        d = Dense(400)(d)\n",
    "        d = Activation('tanh')(d)\n",
    "        d = Dense(1, kernel_initializer='normal', activation='sigmoid')(d)\n",
    "        discriminator = Model(d_input, d)\n",
    "        discriminator.compile(loss='binary_crossentropy', optimizer=dopt)\n",
    "        return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import add, concatenate, multiply\n",
    "mbd=True\n",
    "def GAN_f(generator, discriminator):\n",
    "\n",
    "    noise=Input(shape=[1])\n",
    "    mean = Input(shape=[1])\n",
    "    g=generator(([noise, mean]))\n",
    "    d=discriminator(g)\n",
    "    discriminator.trainable = False\n",
    "    GAN = Model([noise,mean], d)\n",
    "    GAN.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    #GAN.summary()\n",
    "    return GAN\n",
    "\n",
    "\n",
    "D = build_discriminator(d_input=Input(shape=[1]), mbd=True)\n",
    "D.summary()\n",
    "G = gen_f()\n",
    "G.summary()\n",
    "Model = GAN_f(G, D)\n",
    "D.trainable = True\n",
    "Model.summary()\n",
    "#p_g = G.predict(noise(10000))\n",
    "#plt.hist(p_g)\n",
    "#plt.show()\n",
    "#p_d = D.predict(np.ones(5000))\n",
    "#p_d = np.concatenate((p_d, D.predict(np.zeros(5000))))\n",
    "#plt.hist(p_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "import random\n",
    "losses = {\"d_f\":[], \"d_t\":[], \"d\":[], \"g\":[]}\n",
    "def train(G, D, M, epoch, mean):\n",
    "    \n",
    "    true_disc_x = gaussian(batch_size,mu=mean,sigma=0.5)\n",
    "    sd = np.std(true_disc_x)\n",
    "    #mean = np.mean(true_disc_x)\n",
    "    mean_col = mean*np.ones((batch_size,1))\n",
    "    latent = noise_g(batch_size, mu=0., sigma=10.) #noise_g(batch_size, mu=0., sigma = 1.)  # m=true_disc_x.min(), M=true_disc_x.max())\n",
    "    true_disc_y = np.zeros(batch_size)\n",
    "    fake_disc_x = G.predict([latent, mean_col])\n",
    "    #one-sided label smoothing: random.uniform(0.8,1.0)*\n",
    "    fake_disc_y = np.ones(batch_size)\n",
    "    \n",
    "    x_disc = np.concatenate((true_disc_x, fake_disc_x))\n",
    "    x_disc = x_disc.reshape((x_disc.shape[0], 1))\n",
    "    y_disc = np.concatenate((true_disc_y, fake_disc_y))\n",
    "    y_disc = y_disc.reshape((y_disc.shape[0], 1))\n",
    "    \n",
    "    G.trainable = False\n",
    "    D.trainable = True\n",
    "    d_loss_true = D.train_on_batch(true_disc_x, true_disc_y)\n",
    "    d_loss_fake = D.train_on_batch(fake_disc_x, fake_disc_y)\n",
    "    d_loss = (d_loss_true + d_loss_fake) / 2.0\n",
    "    losses[\"d_f\"].append(d_loss_fake)\n",
    "    losses[\"d_t\"].append(d_loss_true)\n",
    "    losses[\"d\"].append(d_loss)\n",
    "    \n",
    "    x_gen_1 = latent\n",
    "    x_gen_2 = mean_col\n",
    "    y_gen = np.zeros(batch_size)    \n",
    "       \n",
    "    G.trainable = True \n",
    "    D.trainable = False    \n",
    "    \n",
    "    GAN_loss = M.train_on_batch([x_gen_1, x_gen_2] , y_gen)\n",
    "    losses[\"g\"].append(GAN_loss)\n",
    "    \n",
    "    return d_loss, GAN_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrain_disc = False\n",
    "if pretrain_disc ==True:\n",
    "    #temp (pretrain dicriminator)\n",
    "    true_disc_x = gaussian(batch_size,mu=0.,sigma=0.5)\n",
    "    mean_col = mean*np.ones((batch_size,1))\n",
    "    latent = noise_g(batch_size, mu=0, sigma=10.)\n",
    "    true_disc_y = np.zeros(batch_size)\n",
    "    fake_disc_x = G.predict([latent, mean_col])\n",
    "    fake_disc_y = np.ones(batch_size)\n",
    "    \n",
    "\n",
    "    #x_disc = np.concatenate((true_disc_x, fake_disc_x))\n",
    "    #x_disc = x_disc.reshape((x_disc.shape[0], 1))\n",
    "    #y_disc = np.concatenate((true_disc_y, fake_disc_y))\n",
    "\n",
    "    G.trainable = False\n",
    "    D.trainable = True\n",
    "    #d_loss_true = D.train_on_batch(true_disc_x, true_disc_y)\n",
    "    #d_loss_fake = D.train_on_batch(fake_disc_x, fake_disc_y)\n",
    "    d_loss = D.train_on_batch(true_disc_x, true_disc_y)\n",
    "    d_loss = D.train_on_batch(fake_disc_x, fake_disc_y)\n",
    "\n",
    "    print d_loss\n",
    "\n",
    "\n",
    "nb_epochs = 5001\n",
    "mean_arr = [-1., 0., 1., 1., 0., -1.]\n",
    "\n",
    "for i in range(0,nb_epochs):\n",
    "    mc=0\n",
    "    for m_true in mean_arr:\n",
    "        d_loss, g_loss = train(G,D,Model,i,m_true)\n",
    "        #losses[\"d\"].append(d_loss)\n",
    "        #losses[\"g\"].append(g_loss)  \n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\"Epoch {0}, Discriminator loss:{1}, GAN loss:{2}\".format(i + 1, d_loss, g_loss))\n",
    "            #overlay GAN output on the Gaussian we want\n",
    "\n",
    "            true_disc = gaussian(10000,mu=m_true,sigma=0.5)\n",
    "            sd = np.std(true_disc)\n",
    "            mean = np.mean(true_disc)\n",
    "            print \"For true: mean and s.d. = \", mean, sd\n",
    "            mean_col = m_true*np.ones((10000,1))\n",
    "            latent = noise_g(10000,mu=0.,sigma=10.)\n",
    "\n",
    "            G.trainable = False\n",
    "            D.trainable = False\n",
    "            fake_disc = G.predict([latent, mean_col])\n",
    "\n",
    "            sd = np.std(fake_disc)\n",
    "            mean = np.mean(fake_disc)\n",
    "            print \"For generator: mean and s.d. = \", mean, sd\n",
    "\n",
    "            plt.figure(figsize=(16,9))\n",
    "            plt.title(\"True (mu=0, sigma=0.5) vs. Generator Output\", fontsize=24)\n",
    "            plt.hist(latent, color='r', histtype='step', linewidth=5, bins = 50, range = (-10,10), label = \"Noise\", alpha=0.5)\n",
    "            plt.hist(true_disc, color='g', histtype='step', linewidth=5, bins = 50, range = (-10,10), label = \"True\", alpha=0.5)\n",
    "            plt.hist(fake_disc, color='b', histtype='step', linewidth=5, bins = 50, range = (-10,10), label = 'Fake/Generator', alpha=0.5)\n",
    "            plt.legend(loc='best', fontsize=22, fancybox=True, framealpha=0.)\n",
    "            # plt.ylim(0,0.6)\n",
    "            plt.rc('xtick', labelsize = 22)\n",
    "            plt.rc('ytick', labelsize = 22)\n",
    "            plt.show()     \n",
    "            #print losses\n",
    "\n",
    "            true_pred = D.predict(true_disc)\n",
    "            fake_pred = D.predict(fake_disc)\n",
    "\n",
    "            plt.figure(figsize=(16,9))\n",
    "            plt.title(\"Discriminator prediction\", fontsize=24)\n",
    "            plt.hist(true_pred, color='r', histtype='step', linewidth=5, bins = 25, range = (-2.,2.), label = \"w/ true input\", alpha=0.5)\n",
    "            plt.hist(fake_pred, color='b', histtype='step', linewidth=5, bins = 25, range = (-2.,2.), label = 'w/ fake input', alpha=0.5)\n",
    "            plt.legend(loc='best', fontsize=22, fancybox=True, framealpha=0.)\n",
    "            plt.xlim(0.,1.)\n",
    "            plt.rc('xtick', labelsize = 22)\n",
    "            plt.rc('ytick', labelsize = 22)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            plot_loss(losses)\n",
    "    #savemodel(model=G, name=\"Gen_mbd_varg_%d\"%mc)\n",
    "    #mc=mc+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = G.predict(noise(batch_size=100000, m=-3., M=3.))\n",
    "plt.hist(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sd = np.std(p, dtype=np.float64)\n",
    "v = np.var(p, dtype=np.float64)\n",
    "mean = np.mean(p, dtype=np.float64)\n",
    "from keras.layers import Activation, Dense\n",
    "print mean, sd\n",
    "savemodel(model=G, name=\"Gen_mbd_varg\")\n",
    "#save_model(model=D, filepath=\"/home/kaustuv1993/Desktop/H2bb/Models_and_Weights/Disc_mbd_1\")\n",
    "\n",
    "#savemodel(model=Model, name=\"GaussiGAN_mbd_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "        ###################\n",
    "        batch_size = 1000000    \n",
    "        true_disc = gaussian(batch_size,mu=2.,sigma=0.5)\n",
    "        true_disc.sort()\n",
    "        print true_disc.min(), true_disc.max()\n",
    "        sd = np.std(true_disc)\n",
    "        mean = 2\n",
    "        mean_col = mean*np.ones((batch_size,1))\n",
    "        latent = noise_g(batch_size,mu=0.,sigma=10.)\n",
    "        G.trainable = False\n",
    "        D.trainable = False\n",
    "        fake_disc = G.predict([latent, mean_col])\n",
    "        latent.sort()\n",
    "        G.trainable = False\n",
    "        D.trainable = False\n",
    "        fake_disc = G.predict([latent,mean_col])\n",
    "        #print fake_disc\n",
    "        sd = np.std(fake_disc)\n",
    "        mean = np.mean(fake_disc)\n",
    "        print \"For generator: mean and s.d = \", mean, sd\n",
    "        ###################\n",
    "        \n",
    "        plt.figure(figsize=(16,9))\n",
    "        plt.title(\"Original (mu=0, sigma=0.5) vs. Generator Output\", fontsize=24)\n",
    "        plt.hist(true_disc, color='r', histtype='step', linewidth=5, bins = 50, range = (-4.5,2.5), label = \"Original\", alpha=0.5, normed=True)\n",
    "        plt.hist(fake_disc, color='b', histtype='step', linewidth=5, bins = 50, range = (-4.5,2.5), label = 'Generator', alpha=0.5, normed=True)\n",
    "        plt.legend(loc='best', fontsize=22, fancybox=True, framealpha=0.)\n",
    "        # plt.ylim(0,0.6)\n",
    "        plt.rc('xtick', labelsize = 22)\n",
    "        plt.rc('ytick', labelsize = 22)\n",
    "        #plt.savefig(\"GaussiGAN_mbd_unilat_gen_PDF.png\")\n",
    "        plt.show()     \n",
    "        #print losses\n",
    "        \n",
    "        true_pred = D.predict(true_disc)\n",
    "        fake_pred = D.predict(fake_disc)\n",
    "            \n",
    "        plt.figure(figsize=(16,9))\n",
    "        plt.title(\"Discriminator prediction\", fontsize=24)\n",
    "        plt.hist(true_pred, color='r', histtype='step', linewidth=5, bins = 10, range = (0.,1.), label = \"w/ true input\", alpha=0.5, normed=True)\n",
    "        plt.hist(fake_pred, color='b', histtype='step', linewidth=5, bins = 10, range = (0.,1.), label = 'w/ fake input', alpha=0.5, normed=True)\n",
    "        plt.legend(loc='best', fontsize=22, fancybox=True, framealpha=0.)\n",
    "        plt.xlim(0.1,0.9)\n",
    "        plt.rc('xtick', labelsize = 22)\n",
    "        plt.rc('ytick', labelsize = 22)\n",
    "        #plt.savefig(\"GaussiGAN_mbd_unilat_disc_pred.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=1000000\n",
    "import scipy.stats as stats        \n",
    "\n",
    "true = gaussian(batch_size=batch_size, mu=0., sigma = 0.5)\n",
    "true.sort()\n",
    "tmean = np.mean(true)\n",
    "tstd = np.std(true)\n",
    "tpdf = stats.norm.pdf(true, tmean, tstd)\n",
    "tcdf = stats.norm.cdf(true, tmean, tstd)\n",
    "#x = np.linspace(true.min(), true.max(), 100000)\n",
    "\n",
    "latent = noise_g(batch_size=batch_size, mu=0., sigma = 1.0)\n",
    "latent.sort()\n",
    "G_x = G.predict(latent)\n",
    "\n",
    "H_G,X_G = np.histogram(G_x, bins = 100, normed = True )\n",
    "dx = X_G[1] - X_G[0]\n",
    "F_G[1:] = np.cumsum(H_G)*dx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.title(\"CDF/PDF overlay\", fontsize=24)\n",
    "\n",
    "plt.plot(true, tpdf, \n",
    "         color='b', linestyle='--', linewidth=5, label = r\"$\\mathrm{True\\ PDF\\ (\\mu=0,\\ \\sigma=0.5)}$\")\n",
    "\n",
    "plt.plot(true, tcdf, \n",
    "         color = 'k', linestyle='--', linewidth=5, label = \"True CDF\")\n",
    "\n",
    "plt.hist(G_x, histtype='step', bins =100, normed=True,\n",
    "         color = 'r', linewidth=3, label =\"Generator PDF\")\n",
    "\n",
    "plt.plot(X_G, F_G, \n",
    "         color = 'y', linewidth=3, label=\"Generator CDF\")\n",
    "\n",
    "plt.legend(loc='best', fontsize=22, fancybox=True, framealpha=0.)\n",
    "plt.rc('xtick', labelsize = 22)\n",
    "plt.rc('ytick', labelsize = 22)\n",
    "plt.savefig(\"GaussiGAN_1_mbd_g1_overlay.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print F_G.shape, X_G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        from scipy.stats import norm\n",
    "        batch_size=100000\n",
    "        \n",
    "        \n",
    "        g_inp = noise_g(batch_size=100000, mu=0., sigma = 1.0)\n",
    "        x = np.linspace(g_inp.min(),g_inp.max(), 100000)\n",
    "        x.sort()\n",
    "        g_inp.sort()\n",
    "        dx = ((x.max()-x.min())/100000.)\n",
    "        G_x = G.predict(g_inp)\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(16,9))\n",
    "        plt.title(\"CDF/PDF overlay\", fontsize=24)\n",
    "        plt.hist(G_x, color='chartreuse', bins=50,  linewidth=3,label = \"Generator predictions\", normed = True, alpha=0.5)\n",
    "        #G_x /= (dx*G_x).sum()\n",
    "        #CDF_gen = np.cumsum(G_x*dx)\n",
    "        plt.plot(g_inp, G_x, color = 'burlywood', linewidth=3, label =\"Generator CDF\")\n",
    "        plt.plot(x, norm.pdf(x, loc = 0., scale = 0.5), color='c', linewidth=3, label = \"True PDF\")\n",
    "        plt.plot(x, norm.cdf(x, loc = 0., scale = 0.5), color = 'm', linewidth=3, label=\"True CDF\")\n",
    "        plt.plot(x, norm.ppf(x, loc = 0., scale = 0.5), color = 'y', linewidth=3, label=\"True PPF\")\n",
    "        plt.plot(g_inp, norm.pdf(G_x, loc = 0., scale = 0.5), color = 'r', linestyle = '--', linewidth=3, label=\"Gen. PDF\")\n",
    "        plt.plot(g_inp, norm.cdf(G_x, loc = 0., scale = 0.5), color = 'g', linestyle = '--', linewidth=3, label=\"Gen. CDF\")\n",
    "        plt.plot(g_inp, norm.ppf(G_x, loc = 0., scale = 0.5), color = 'b', linestyle = '--', linewidth=3, label=\"Gen. PPF\")\n",
    "        plt.legend(loc='best', fontsize=22, fancybox=True, framealpha=0.)\n",
    "        #plt.ylim(-2,2)\n",
    "        plt.rc('xtick', labelsize = 22)\n",
    "        plt.rc('ytick', labelsize = 22)\n",
    "        plt.savefig(\"GaussiGAN_1_mbd_g1_overlay.png\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(G, show_shapes=True, to_file='VarGaussiGen.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "RANGE = 5\n",
    "\n",
    "\n",
    "# p_d(x)\n",
    "class DataDistribution(object):\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def sample(self, N):\n",
    "        samples = np.random.normal(self.mu, self.sigma, N)\n",
    "        samples.sort()\n",
    "        return samples\n",
    "\n",
    "\n",
    "# p_z(z)\n",
    "class NoiseDistribution(object):\n",
    "    def __init__(self, range):\n",
    "        self.range = range\n",
    "\n",
    "    # equally spaced samples + noise\n",
    "    def sample(self, N):\n",
    "        offset = np.random.random(N) * (float(self.range) / N)\n",
    "        samples = np.linspace(-self.range, self.range, N) + offset\n",
    "        return samples\n",
    "\n",
    "\n",
    "# G(z)\n",
    "def generator(x, n_hidden=32):\n",
    "\n",
    "    # initializers\n",
    "    w_init = tf.truncated_normal_initializer(stddev=2)\n",
    "    b_init = tf.constant_initializer(0.)\n",
    "\n",
    "    # 1st hidden layer\n",
    "    w0 = tf.get_variable('w0', [x.get_shape()[1], n_hidden], initializer=w_init)\n",
    "    b0 = tf.get_variable('b0', [n_hidden], initializer=b_init)\n",
    "    h0 = tf.nn.relu(tf.matmul(x, w0) + b0)\n",
    "\n",
    "    # output layer\n",
    "    w1 = tf.get_variable('w1', [h0.get_shape()[1], 1], initializer=w_init)\n",
    "    b1 = tf.get_variable('b1', [1], initializer=b_init)\n",
    "    o = tf.matmul(h0, w1) + b1\n",
    "\n",
    "    return o\n",
    "\n",
    "\n",
    "# D(x)\n",
    "def discriminator(x, n_hidden=32):\n",
    "\n",
    "    # initializers\n",
    "    w_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    b_init = tf.constant_initializer(0.)\n",
    "\n",
    "    # 1st hidden layer\n",
    "    w0 = tf.get_variable('w0', [x.get_shape()[1], n_hidden], initializer=w_init)\n",
    "    b0 = tf.get_variable('b0', [n_hidden], initializer=b_init)\n",
    "    h0 = tf.nn.relu(tf.matmul(x, w0) + b0)\n",
    "\n",
    "    # output layer\n",
    "    w1 = tf.get_variable('w1', [h0.get_shape()[1], 1], initializer=w_init)\n",
    "    b1 = tf.get_variable('b1', [1], initializer=b_init)\n",
    "    o = tf.sigmoid(tf.matmul(h0, w1) + b1)\n",
    "\n",
    "    return o\n",
    "\n",
    "\n",
    "# re-used for optimizing all networks\n",
    "def optimizer(loss, var_list, num_decay_steps=400, initial_learning_rate=0.03):\n",
    "    decay = 0.95\n",
    "    batch = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        initial_learning_rate,\n",
    "        batch,\n",
    "        num_decay_steps,\n",
    "        decay,\n",
    "        staircase=True\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "        loss,\n",
    "        global_step=batch,\n",
    "        var_list=var_list\n",
    "    )\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "# plot decision boundaries (init, pre-trained, trained) and p_data, p_g\n",
    "class ResultPlot(object):\n",
    "    def __init__(self, num_points, num_bins, mu, sigma):\n",
    "        self.num_points = num_points    # number of data points to be evaluated\n",
    "        self.num_bins = num_bins        # number of bins to get histogram\n",
    "\n",
    "        self.mu = mu                    # mu of p_data\n",
    "        self.sigma = sigma              # sigma of p_data\n",
    "\n",
    "        self.xs = np.linspace(-RANGE, RANGE, num_points)    # positions of data\n",
    "        self.bins = np.linspace(-RANGE, RANGE, num_bins)    # positions of bins\n",
    "\n",
    "    def show_results(self, db_init, db_pre_trained, db_trained, pd, pg, save_img=True):\n",
    "        db_x = np.linspace(-RANGE, RANGE, len(db_trained))\n",
    "        p_x = np.linspace(-RANGE, RANGE, len(pd))\n",
    "        f, ax = plt.subplots(1)\n",
    "        ax.plot(db_x, db_init, 'g--', linewidth=2, label='db_init')\n",
    "        ax.plot(db_x, db_pre_trained, 'c--', linewidth=2, label='db_pre_trained')\n",
    "        ax.plot(db_x, db_trained, 'g-', linewidth=2, label='db_trained')\n",
    "        ax.set_ylim(0, max(1, np.max(pd) * 1.1))\n",
    "        ax.set_xlim(max(self.mu - self.sigma * 3, -RANGE * 0.9), min(self.mu + self.sigma * 3, RANGE * 0.9))\n",
    "        plt.plot(p_x, pd, 'b-', linewidth=2, label='real data')\n",
    "        plt.plot(p_x, pg, 'r-', linewidth=2, label='generated data')\n",
    "        plt.title('1D Generative Adversarial Network: ' + '(mu : %3g,' % self.mu + ' sigma : %3g)' % self.sigma)\n",
    "        plt.xlabel('Data values')\n",
    "        plt.ylabel('Probability density')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_img:\n",
    "            plt.savefig('GAN_1d_gaussian' + '_mu_%g' % self.mu + '_sigma_%g' % self.sigma + '.png')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" parameters \"\"\"\n",
    "    # p_data\n",
    "    mu = 0\n",
    "    sigma = 1\n",
    "\n",
    "    # training\n",
    "    B = 150                 # batch-size\n",
    "    LR = 0.03               # learning rate (generator uses half of this)\n",
    "    TRAIN_ITERS = 3000      # number of iterations\n",
    "\n",
    "    # network\n",
    "    n_hidden = 32           # number of hidden layers\n",
    "\n",
    "    \"\"\" build graph \"\"\"\n",
    "\n",
    "    # networks : pre-trained discriminator\n",
    "    with tf.variable_scope('D_pre'):\n",
    "        pre_input = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        pre_labels = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        D_pre = discriminator(pre_input, n_hidden)\n",
    "\n",
    "    # networks : generator\n",
    "    with tf.variable_scope('Gen'):\n",
    "        z = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        G_z = generator(z, n_hidden)\n",
    "\n",
    "    # networks : discriminator\n",
    "    with tf.variable_scope('Disc') as scope:\n",
    "        x = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        D_real = discriminator(x, n_hidden)\n",
    "        scope.reuse_variables()\n",
    "        D_fake = discriminator(G_z, n_hidden)\n",
    "\n",
    "    # loss for each network\n",
    "    eps = 1e-2  # to prevent log(0) case\n",
    "    pre_loss = tf.reduce_mean(tf.square(D_pre - pre_labels))\n",
    "    loss_g = tf.reduce_mean(-tf.log(D_fake + eps))\n",
    "    loss_d = tf.reduce_mean(-tf.log(D_real + eps) - tf.log(1 - D_fake + eps))\n",
    "\n",
    "    # trainable variables for each network\n",
    "    d_pre_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D_pre')\n",
    "    d_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Disc')\n",
    "    g_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Gen')\n",
    "\n",
    "    # optimizer for each network\n",
    "    pre_opt = optimizer(pre_loss, d_pre_params, 400, LR)\n",
    "    opt_d = optimizer(loss_d, d_params, 400, LR)\n",
    "    opt_g = optimizer(loss_g, g_params, 400, LR / 2)\n",
    "\n",
    "    \"\"\" training \"\"\"\n",
    "\n",
    "    # open session and initialize all variables\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # sources\n",
    "    p_data = DataDistribution(mu, sigma)\n",
    "    p_z = NoiseDistribution(RANGE)\n",
    "\n",
    "    # class for result-plot\n",
    "    plot = ResultPlot(10000, 20, mu, sigma)\n",
    "\n",
    "    # plot : initial decision boundary\n",
    "    db_init = np.zeros((plot.num_points, 1))\n",
    "    for i in range(plot.num_points // B):\n",
    "        db_init[B * i:B * (i + 1)] = sess.run(D_real, {x: np.reshape(plot.xs[B * i:B * (i + 1)], (B, 1))})\n",
    "\n",
    "    # pre-training discriminator\n",
    "    num_pretrain_steps = 1000\n",
    "    for step in range(num_pretrain_steps):\n",
    "\n",
    "        print('pre-training :  %d/%d' % (step + 1, num_pretrain_steps))\n",
    "\n",
    "        # Object of pre-training is to make decision boundary as similar as pdf of data (i.e. p_data)\n",
    "        # Since p_data is unknown in real situation, we get histogram and estimate pdf\n",
    "        N = 1000\n",
    "        d = p_data.sample(N)\n",
    "        n_bins = 100\n",
    "        histc, edges = np.histogram(d, bins=n_bins, density=True)\n",
    "\n",
    "        # Estimated pdf is used as labels after normalization\n",
    "        max_histc = np.max(histc)\n",
    "        min_histc = np.min(histc)\n",
    "        labels = (histc - min_histc) / (max_histc - min_histc)\n",
    "        d = edges[1:]\n",
    "\n",
    "        # Execute one training step\n",
    "        sess.run([pre_loss, pre_opt],\n",
    "                 {pre_input: np.reshape(d, (n_bins, 1)), pre_labels: np.reshape(labels, (n_bins, 1))})\n",
    "    print('pre-training finished!')\n",
    "\n",
    "    # store pre-trained variables\n",
    "    weightsD = sess.run(d_pre_params)\n",
    "\n",
    "    # copy weights from pre-training over to new D network\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i, v in enumerate(d_params):\n",
    "        sess.run(v.assign(weightsD[i]))\n",
    "\n",
    "    # plot : pre-trained decision boundary\n",
    "    db_pre_trained = np.zeros((plot.num_points, 1))\n",
    "    for i in range(plot.num_points // B):\n",
    "        db_pre_trained[B * i:B * (i + 1)] = sess.run(D_real, {x: np.reshape(plot.xs[B * i:B * (i + 1)], (B, 1))})\n",
    "\n",
    "    # training-loop\n",
    "    for step in range(TRAIN_ITERS):\n",
    "\n",
    "        np.random.seed(np.random.randint(0, TRAIN_ITERS))\n",
    "\n",
    "        # update discriminator\n",
    "        x_ = p_data.sample(B)\n",
    "        z_ = p_z.sample(B)\n",
    "\n",
    "        loss_d_, _ = sess.run([loss_d, opt_d], {x: np.reshape(x_, (B, 1)), z: np.reshape(z_, (B, 1))})\n",
    "\n",
    "        # update generator\n",
    "        z_ = p_z.sample(B)\n",
    "        loss_g_, _ = sess.run([loss_g, opt_g], {z: np.reshape(z_, (B, 1))})\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('[%d/%d]: loss_d : %.3f, loss_g : %.3f' % (step, TRAIN_ITERS, loss_d_, loss_g_))\n",
    "\n",
    "    \"\"\" show results \"\"\"\n",
    "\n",
    "    # plot : trained decision boundary\n",
    "    db_trained = np.zeros((plot.num_points, 1))\n",
    "    for i in range(plot.num_points // B):\n",
    "        db_trained[B * i:B * (i + 1)] = sess.run(D_real, {x: np.reshape(plot.xs[B * i:B * (i + 1)], (B, 1))})\n",
    "\n",
    "    # plot : pdf of data distribution\n",
    "    d = p_data.sample(plot.num_points)\n",
    "    pd, _ = np.histogram(d, bins=plot.bins, density=True)\n",
    "\n",
    "    # plot : pdf of generated samples\n",
    "    zs = np.linspace(-RANGE, RANGE, plot.num_points)\n",
    "    g = np.zeros((plot.num_points, 1))\n",
    "    for i in range(plot.num_points // B):\n",
    "        g[B * i:B * (i + 1)] = sess.run(G_z, {z: np.reshape(zs[B * i:B * (i + 1)], (B, 1))})\n",
    "    pg, _ = np.histogram(g, bins=plot.bins, density=True)\n",
    "\n",
    "    # plot results\n",
    "    plot.show_results(db_init, db_pre_trained, db_trained, pd, pg, save_img=True)\n",
    "\n",
    "    sess.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
